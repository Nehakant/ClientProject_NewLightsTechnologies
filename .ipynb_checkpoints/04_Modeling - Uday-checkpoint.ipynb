{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "# Machine learning relevant libraries\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import StratifiedShuffleSplit,GroupShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the compiled csv of all different tweets\n",
    "df = pd.read_csv('./data/all_disaster_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet text</th>\n",
       "      <th>disaster type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>HOY es nuestro dia @Maria_21_music  #nerea #an...</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Yeahh yeahh esa jenga @alejandroalija @Telarer...</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>a Mãe do meu amigo #Pablo. morreu :( ta muito ...</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Madrinas y apadrinados #SantaCecilia2012 @Carm...</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11-26-12 | E-Games Attendant | Chada Bingo Hou...</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet text  disaster type\n",
       "0  HOY es nuestro dia @Maria_21_music  #nerea #an...           99.0\n",
       "1  Yeahh yeahh esa jenga @alejandroalija @Telarer...           99.0\n",
       "2  a Mãe do meu amigo #Pablo. morreu :( ta muito ...           99.0\n",
       "3  Madrinas y apadrinados #SantaCecilia2012 @Carm...           99.0\n",
       "4  11-26-12 | E-Games Attendant | Chada Bingo Hou...           99.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet text        0\n",
       "disaster type    10\n",
       "dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for nulls\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping NaNs\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn all the tweets into lowercase before vectorization\n",
    "df['tweet text'] = df['tweet text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a column identifying floods and no floods to create a one-class classifier\n",
    "df['flood'] = df['disaster type'].apply(lambda x: 1 if x == 4 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the X and y lists before splitting\n",
    "X = df['tweet text']\n",
    "y = df['flood']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split for single classifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for single classifier\n",
    "pipe1 = Pipeline([\n",
    "    \n",
    "    ('cvec',CountVectorizer()),\n",
    "    ('lr',LogisticRegression(penalty = 'l2', solver='liblinear', random_state = 42))\n",
    "    \n",
    "])\n",
    "\n",
    "pipe1_params = {\n",
    "    \n",
    "    'cvec__max_features': [2000],\n",
    "    'cvec__lowercase': [True],\n",
    "    'cvec__stop_words': ['english'],\n",
    "    'cvec__ngram_range': [(1, 1),(1, 2)],\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid searching for single classifier\n",
    "gs1 = GridSearchCV(pipe1,\n",
    "                  pipe1_params,\n",
    "                  cv=3,\n",
    "                  return_train_score=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "gs1.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__lowercase': True,\n",
       " 'cvec__max_features': 2000,\n",
       " 'cvec__ngram_range': (1, 1),\n",
       " 'cvec__stop_words': 'english'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the best parameters\n",
    "gs1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVEC/LR Training Score: 0.9874934364408803\n"
     ]
    }
   ],
   "source": [
    "# Score the training data\n",
    "print(f'CVEC/LR Training Score: {gs1.score(X_train, y_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVEC/LR Testing Score: 0.9785223367697594\n"
     ]
    }
   ],
   "source": [
    "# Score the testing data\n",
    "print(f'CVEC/LR Testing Score: {gs1.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Multiclass Classification with 5 Disaster Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the y vairable as a vector of different disaster types\n",
    "y_multi = df['disaster type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_multi, random_state = 42, stratify = y_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a pipeline to vectorize and classify the text data\n",
    "pipe2 = Pipeline([\n",
    "    \n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('multi', OneVsRestClassifier(LogisticRegression(penalty = 'l2', solver='liblinear', random_state = 42)))\n",
    "    \n",
    "])\n",
    "\n",
    "# Parameter dictionary to use for gridsearching\n",
    "pipe2_params = {\n",
    "    \n",
    "    'tvec__max_features':[2000, 5000],\n",
    "    'tvec__lowercase':[True],\n",
    "    'tvec__stop_words':['english'],\n",
    "    'tvec__ngram_range':[(1, 1),(1, 2)]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gridsearch over the parameter dictionary associated with the above pipeline\n",
    "gs2 = GridSearchCV(pipe2,\n",
    "                  pipe2_params,\n",
    "                  cv=3,\n",
    "                  return_train_score=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "gs2.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tvec__lowercase': True,\n",
       " 'tvec__max_features': 5000,\n",
       " 'tvec__ngram_range': (1, 2),\n",
       " 'tvec__stop_words': 'english'}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the best model parameters\n",
    "gs2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF/OVR Training Score: 0.9393288462456442\n"
     ]
    }
   ],
   "source": [
    "# Find the training score\n",
    "print(f'TFIDF/OVR Training Score: {gs2.score(X_train, y_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF/OVR Testing Score: 0.9213917525773195\n"
     ]
    }
   ],
   "source": [
    "# Find the testing score\n",
    "print(f'TFIDF/OVR Testing Score: {gs2.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "preds = gs2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create testing DataFrame\n",
    "df_test = df.loc[list(X_test.index),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with predicted disaster type from our multiclass classifier\n",
    "df_test['preds disaster type'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet text</th>\n",
       "      <th>disaster type</th>\n",
       "      <th>preds disaster type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>17640</td>\n",
       "      <td>rt @examinedisaster: @nasa releases stunning b...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3005</td>\n",
       "      <td>dramatic details emerge of lax rampage targeti...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16996</td>\n",
       "      <td>rt @estebangerbasi: un terrorista italiano es ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1687</td>\n",
       "      <td>photo: kirsty nelis, reported locally as polic...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20253</td>\n",
       "      <td>deadly building collapse in bangladesh  http:/...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10676</td>\n",
       "      <td>colorado digs out after 'biblical' flooding: c...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1811</td>\n",
       "      <td>nasty katie hopkins apologises after making cr...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006</td>\n",
       "      <td>rt @cloydrivers: bad day? 6 troops in afghanis...</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19090</td>\n",
       "      <td>rt @fasnouvelles: vous avez des livres à donne...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26063</td>\n",
       "      <td>rt @ellendatlow: re: metro north derailment:jf...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6984 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweet text  disaster type  \\\n",
       "17640  rt @examinedisaster: @nasa releases stunning b...            0.0   \n",
       "3005   dramatic details emerge of lax rampage targeti...            3.0   \n",
       "16996  rt @estebangerbasi: un terrorista italiano es ...            3.0   \n",
       "1687   photo: kirsty nelis, reported locally as polic...            3.0   \n",
       "20253  deadly building collapse in bangladesh  http:/...            3.0   \n",
       "...                                                  ...            ...   \n",
       "10676  colorado digs out after 'biblical' flooding: c...            4.0   \n",
       "1811   nasty katie hopkins apologises after making cr...            3.0   \n",
       "2006   rt @cloydrivers: bad day? 6 troops in afghanis...           99.0   \n",
       "19090  rt @fasnouvelles: vous avez des livres à donne...            3.0   \n",
       "26063  rt @ellendatlow: re: metro north derailment:jf...            3.0   \n",
       "\n",
       "       preds disaster type  \n",
       "17640                  0.0  \n",
       "3005                   3.0  \n",
       "16996                  3.0  \n",
       "1687                   3.0  \n",
       "20253                  3.0  \n",
       "...                    ...  \n",
       "10676                  4.0  \n",
       "1811                   3.0  \n",
       "2006                  99.0  \n",
       "19090                  3.0  \n",
       "26063                  3.0  \n",
       "\n",
       "[6984 rows x 3 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select tweet text, disaster type and predictions\n",
    "df_test[['tweet text','disaster type','preds disaster type']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Multiclass Classification Performed on Wildfire Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the dataframe of tweets from california\n",
    "df_fire = pd.read_csv('./data/firetweets_clean.csv')\n",
    "#df_fire = df_fire[['message']].rename({'message':'tweet'}, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet text</th>\n",
       "      <th>disaster type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>california #artist danielle nelisse paints #ab...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>many low income people lost homes baja califor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>rt epochtimes get act together governor see cl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>rt epochtimes get act together governor see cl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>yikes https co gs vp hh #wildfires #africa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet text  disaster type\n",
       "0  california #artist danielle nelisse paints #ab...              0\n",
       "1  many low income people lost homes baja califor...              0\n",
       "2  rt epochtimes get act together governor see cl...              0\n",
       "3  rt epochtimes get act together governor see cl...              0\n",
       "4         yikes https co gs vp hh #wildfires #africa              0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display data\n",
    "df_fire.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6260, 2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get shape of firetweets\n",
    "df_fire.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2792, 2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are many repetitive tweets, so let us drop and keep one\n",
    "df_fire.drop_duplicates(subset='tweet text', keep='first', inplace=True)\n",
    "df_fire.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing all tweets to lowercase\n",
    "df_fire['tweet text'] = df_fire['tweet text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the multiclass classifier model to the wildfire tweets\n",
    "df_fire['predicted_disaster_type'] = gs2.predict(df_fire['tweet text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0     0.661891\n",
       "99.0    0.162607\n",
       "4.0     0.088109\n",
       "3.0     0.085960\n",
       "5.0     0.001433\n",
       "Name: predicted_disaster_type, dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentage of tweets based on category\n",
    "df_fire['predicted_disaster_type'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There are no type 1 and type 2 disaster, which according to our dictionary is cyclones/hurricanes and earthquakes respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#russia love california dreaming president donald trump seems willing help russia fight wi https co fy izy xqq\n",
      "\n",
      "rt enviroedgenews declare clearly unequivocally #planet #earth facing #climateemergency scientists co\n",
      "\n",
      "bad year #wildfires globally past months parts lebanon turkey russia france https co xyvwcgfuqh\n",
      "\n",
      "rt benjonespiced mesmerising image kentphotos via ap meteor streaks across sky gusty winds create ember cast val\n"
     ]
    }
   ],
   "source": [
    "# Exploring tweets from different categories\n",
    "for tweet in df_fire[df_fire['predicted_disaster_type'] == 5]['tweet text']:\n",
    "    print()\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised clustering to increase dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_multi, random_state = 42, stratify = y_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating a KMeans clustering algorithm\n",
    "km = KMeans(n_clusters=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing the training and testing tweets\n",
    "cvec= CountVectorizer(lowercase=True,stop_words='english',max_features=4000)\n",
    "\n",
    "# Creating vectorized model variables\n",
    "X_train_vec = cvec.fit_transform(X_train)\n",
    "X_test_vec = cvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the vectorized training data\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Transformed the testing data\n",
    "X_train_vec = sc.fit_transform(X_train_vec.toarray())\n",
    "X_test_vec = sc.transform(X_test_vec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames for the vectorized training and testing data\n",
    "df_Xvec_train = pd.DataFrame(X_train_vec)\n",
    "df_Xvec_test = pd.DataFrame(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=4, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the clustering model on the training set\n",
    "km.fit(X_train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the cluster labels to the dataframes\n",
    "df_Xvec_train['cluster'] = km.predict(X_train_vec)\n",
    "df_Xvec_test['cluster'] = km.predict(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy the training and testing cluster columns\n",
    "df_Xvec_train = pd.get_dummies(df_Xvec_train, columns=['cluster'], drop_first=True)\n",
    "df_Xvec_test = pd.get_dummies(df_Xvec_test, columns=['cluster'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='warn',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=42,\n",
       "                                                 solver='liblinear', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False),\n",
       "                    n_jobs=None)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting a multiclass logisitic regression model\n",
    "multi_model = OneVsRestClassifier(LogisticRegression(penalty = 'l2', solver='liblinear', random_state = 42))\n",
    "multi_model.fit(df_Xvec_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9884958709246264"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training score\n",
    "multi_model.score(df_Xvec_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 4002 features per sample; expecting 4003",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-f8c70f448b8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Testing score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmulti_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_Xvec_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    355\u001b[0m         \"\"\"\n\u001b[0;32m    356\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\multiclass.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    300\u001b[0m             \u001b[0margmaxima\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                 \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_predict_binary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxima\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxima\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m                 \u001b[0margmaxima\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmaxima\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\multiclass.py\u001b[0m in \u001b[0;36m_predict_binary\u001b[1;34m(estimator, X)\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;31m# probabilities of the positive class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[1;32m--> 270\u001b[1;33m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[1;31mValueError\u001b[0m: X has 4002 features per sample; expecting 4003"
     ]
    }
   ],
   "source": [
    "# Testing score\n",
    "multi_model.score(df_Xvec_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
